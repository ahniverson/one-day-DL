{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hukim1112/one-day-DL/blob/main/regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pZ8A2liqvgk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cweoTiruj8O"
      },
      "source": [
        "## IMDB 데이터셋 다운로드\n",
        "\n",
        "이전 노트북에서처럼 임베딩을 사용하지 않고 여기에서는 문장을 멀티-핫 인코딩(multi-hot encoding)으로 변환하겠습니다. 이 모델은 훈련 세트에 빠르게 과대적합될 것입니다. 과대적합을 발생시키기고 어떻게 해결하는지 보이기 위해 선택했습니다.\n",
        "\n",
        "멀티-핫 인코딩은 정수 시퀀스를 0과 1로 이루어진 벡터로 변환합니다. 정확하게 말하면 시퀀스 `[3, 5]`를 인덱스 3과 5만 1이고 나머지는 모두 0인 10,00 차원 벡터로 변환한다는 의미입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpzE4iqZtJly"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # NumPy를 가져와서 다차원 배열 연산을 수행합니다.\n",
        "from tensorflow import keras  # TensorFlow의 Keras를 가져옵니다.\n",
        "\n",
        "def multi_hot_sequences(sequences, dimension):\n",
        "    \"\"\"\n",
        "    정수 인덱스 시퀀스를 멀티 핫 인코딩 벡터로 변환합니다.\n",
        "\n",
        "    Args:\n",
        "        sequences (list of lists): 각 샘플이 단어 인덱스로 이루어진 리스트.\n",
        "        dimension (int): 사용할 단어 집합 크기 (단어 인덱스 범위).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: (num_samples, dimension) 형태의 다차원 배열.\n",
        "                    해당 단어 인덱스 위치는 1, 나머지는 0.\n",
        "    \"\"\"\n",
        "    # (샘플 개수, 단어 집합 크기) 크기의 0으로 채워진 행렬을 만듭니다.\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "\n",
        "    # 각 샘플(리뷰)의 단어 인덱스를 멀티 핫 벡터로 변환\n",
        "    for i, word_indices in enumerate(sequences):\n",
        "        results[i, word_indices] = 1.0  # 특정 단어 인덱스에 해당하는 위치를 1로 설정\n",
        "    return results\n",
        "\n",
        "# 사용할 단어 집합의 최대 크기 (최상위 1000개의 단어만 유지)\n",
        "NUM_WORDS = 1000\n",
        "\n",
        "# IMDb 데이터셋 로드 (num_words=NUM_WORDS: 자주 등장하는 1000개의 단어만 유지)\n",
        "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)\n",
        "\n",
        "# 데이터셋을 멀티 핫 인코딩 벡터로 변환\n",
        "train_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)\n",
        "test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS)\n",
        "\n",
        "# 출력 형태 확인\n",
        "print(\"훈련 데이터 형태:\", train_data.shape)  # (샘플 수, 1000)\n",
        "print(\"테스트 데이터 형태:\", test_data.shape)  # (샘플 수, 1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzWVeXe3NBTn"
      },
      "source": [
        "만들어진 멀티-핫 벡터 중 하나를 살펴 보죠. 단어 인덱스는 빈도 순으로 정렬되어 있습니다. 그래프에서 볼 수 있듯이 인덱스 0에 가까울수록 1이 많이 등장합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71kr5rG4LkGM"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 학습데이터와 테스트데이터를 토치의 데이터로더로 변환하겠습니다."
      ],
      "metadata": {
        "id": "8GIbvU9I7gxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 훈련 및 테스트 데이터를 PyTorch Tensor로 변환하여 TensorDataset 생성\n",
        "train_dataset = TensorDataset(\n",
        "    torch.tensor(train_data, dtype=torch.float32),  # 입력 데이터 (멀티 핫 벡터)\n",
        "    torch.tensor(train_labels.reshape(-1, 1), dtype=torch.float32)  # 레이블 (이진 분류)\n",
        ")\n",
        "\n",
        "test_dataset = TensorDataset(\n",
        "    torch.tensor(test_data, dtype=torch.float32),\n",
        "    torch.tensor(test_labels.reshape(-1, 1), dtype=torch.float32)\n",
        ")"
      ],
      "metadata": {
        "id": "WOVYxWEjOrhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch 데이터로더를 구축합니다. 배치사이즈는 512개, 학습셋은 shuffe해줍니다.\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=512, shuffle=True)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=512, shuffle=False)"
      ],
      "metadata": {
        "id": "kkHCv56f7zGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로더 샘플 확인\n",
        "for batch in train_dataloader:\n",
        "    x_batch, y_batch = batch\n",
        "    print(\"입력 데이터 배치 크기:\", x_batch.shape)  # (batch_size, NUM_WORDS)\n",
        "    print(\"레이블 배치 크기:\", y_batch.shape)  # (batch_size, 1)\n",
        "    break  # 첫 번째 배치만 출력"
      ],
      "metadata": {
        "id": "ukKcQUc_8LxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 코드를 작성합니다."
      ],
      "metadata": {
        "id": "xy2bf1rbCZFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from matplotlib.colors import XKCD_COLORS  # (사용되지 않음 - 필요 없으면 제거 가능)\n",
        "\n",
        "# 사용할 디바이스 설정 (CUDA 사용 가능 여부 확인)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "def train_epoch(dataloader, model, loss_fn, optimizer):\n",
        "    \"\"\"\n",
        "    한 에포크(epoch) 동안 모델을 학습하는 함수\n",
        "\n",
        "    Args:\n",
        "        dataloader (DataLoader): 훈련 데이터 로더\n",
        "        model (nn.Module): 학습할 신경망 모델\n",
        "        loss_fn (torch.nn): 손실 함수 (BCELoss 사용)\n",
        "        optimizer (torch.optim): 옵티마이저 (Adam, SGD 등)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (평균 손실, 정확도)\n",
        "    \"\"\"\n",
        "    size = len(dataloader.dataset)  # 전체 데이터 개수\n",
        "    num_batches = len(dataloader)  # 미니배치 개수\n",
        "    model.train()  # 모델을 학습 모드로 설정\n",
        "    total_loss, correct = 0, 0  # 손실과 정확도 초기화\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)  # 입력 데이터와 라벨을 GPU/CPU로 이동\n",
        "\n",
        "        # 모델 예측\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)  # 손실 계산\n",
        "        total_loss += loss.item()  # 손실 누적\n",
        "        correct += ((pred > 0.5) == y).sum().item()  # 정확도 계산 (이진 분류)\n",
        "\n",
        "        # 역전파 (Backpropagation)\n",
        "        optimizer.zero_grad()  # 기존 그래디언트 초기화\n",
        "        loss.backward()  # 그래디언트 계산\n",
        "        optimizer.step()  # 가중치 업데이트\n",
        "\n",
        "        # 100 배치마다 손실 출력\n",
        "        if batch % 100 == 0:\n",
        "            loss_val, current = loss.item(), batch * len(X)\n",
        "            print(f\"Loss: {loss_val:.6f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    # 평균 손실 및 정확도 계산\n",
        "    total_loss /= num_batches\n",
        "    correct /= size\n",
        "    return total_loss, correct  # (평균 손실, 정확도)\n",
        "\n",
        "def test_epoch(dataloader, model, loss_fn):\n",
        "    \"\"\"\n",
        "    한 에포크(epoch) 동안 모델을 검증하는 함수\n",
        "\n",
        "    Args:\n",
        "        dataloader (DataLoader): 검증 데이터 로더\n",
        "        model (nn.Module): 평가할 신경망 모델\n",
        "        loss_fn (torch.nn): 손실 함수 (BCELoss 사용)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (평균 손실, 정확도)\n",
        "    \"\"\"\n",
        "    size = len(dataloader.dataset)  # 전체 데이터 개수\n",
        "    num_batches = len(dataloader)  # 미니배치 개수\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "    total_loss, correct = 0, 0  # 손실과 정확도 초기화\n",
        "\n",
        "    with torch.no_grad():  # 그래디언트 계산 비활성화 (메모리 절약)\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X, y = X.to(device), y.to(device)  # 입력 데이터와 라벨을 GPU/CPU로 이동\n",
        "\n",
        "            # 모델 예측\n",
        "            pred = model(X)\n",
        "            loss = loss_fn(pred, y)  # 손실 계산\n",
        "            total_loss += loss.item()  # 손실 누적\n",
        "            correct += ((pred > 0.5) == y).sum().item()  # 정확도 계산\n",
        "\n",
        "    # 평균 손실 및 정확도 계산\n",
        "    total_loss /= num_batches\n",
        "    correct /= size\n",
        "    return total_loss, correct  # (평균 손실, 정확도)\n",
        "\n",
        "def train_and_test(model, optimizer):\n",
        "    \"\"\"\n",
        "    모델을 학습하고 검증하는 함수\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): 학습할 신경망 모델\n",
        "        optimizer (torch.optim): 옵티마이저 (Adam, SGD 등)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (훈련 손실 리스트, 검증 손실 리스트)\n",
        "    \"\"\"\n",
        "    epochs = 20  # 총 학습 에포크 수\n",
        "    train_losses = []  # 훈련 손실 저장 리스트\n",
        "    val_losses = []  # 검증 손실 저장 리스트\n",
        "    loss_fn = nn.BCELoss()  # 이진 분류 손실 함수 (Binary Cross Entropy Loss)\n",
        "\n",
        "    for t in range(epochs):\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "\n",
        "        # 훈련 실행\n",
        "        train_loss, train_acc = train_epoch(train_dataloader, model, loss_fn, optimizer)\n",
        "        print(f\"Train Accuracy: {(100 * train_acc):>0.1f}%, Avg Loss: {train_loss:.6f}\")\n",
        "\n",
        "        # 검증 실행\n",
        "        val_loss, val_acc = test_epoch(test_dataloader, model, loss_fn)\n",
        "        print(f\"Validation Accuracy: {(100 * val_acc):>0.1f}%, Avg Loss: {val_loss:.6f}\")\n",
        "\n",
        "        # 손실 저장\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "    print(\"Training Complete!\")\n",
        "    return train_losses, val_losses  # 학습 및 검증 손실 반환\n"
      ],
      "metadata": {
        "id": "BMJl_gzhCYp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 과대적합(Overfitting)\n",
        "\n",
        "과대적합은 딥러닝 모델이 훈련 데이터에는 높은 성능을 보이지만, 테스트 데이터에서는 일반화되지 못하는 현상입니다. 이를 방지하는 가장 효과적인 방법은 더 많은 데이터를 확보하는 것입니다. 여의치 않다면, 또 다른 방법은 모델의 규모를 축소하는 것입니다. 즉, 모델에 있는 학습 가능한 파라미터의 수를 줄입니다.\n",
        "\n",
        "딥러닝에서는 모델의 학습 가능한 파라미터의 수를 종종 모델의 \"용량\"이라고 말합니다. 직관적으로 생각해 보면 많은 파라미터를 가진 모델이 더 많은 \"기억 용량\"을 가집니다.\n",
        "\n",
        "- 모델이 너무 크면 훈련 데이터의 패턴을 암기해버려 테스트 데이터에 일반화되지 않습니다.\n",
        "- 반대로 모델이 너무 작으면 복잡한 패턴을 학습하지 못해 성능이 떨어집니다.\n",
        "\n",
        "\n",
        "즉 \"너무 많은 용량\"과 \"충분하지 않은 용량\" 사이의 균형을 잡아야 합니다. 안타깝지만 어떤 모델의 (층의 개수나 뉴런 개수에 해당하는) 적절한 크기나 구조를 결정하는 마법같은 공식은 없습니다. 여러 가지 다른 구조를 사용해 실험을 해봐야만 합니다.\n"
      ],
      "metadata": {
        "id": "crKkAI10Oexf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ReKHdC2EgVu"
      },
      "source": [
        "### 기준 모델 만들기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fcn = nn.Sequential(\n",
        "            nn.Linear(1000, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.fcn(x)\n",
        "        y = torch.sigmoid(f)\n",
        "        return y"
      ],
      "metadata": {
        "id": "guKU5ov46SP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BaselineModel().to(device)\n",
        "baseline_train_losses, baseline_test_losses = train_and_test(model, torch.optim.Adam(model.parameters(), lr=1e-3))"
      ],
      "metadata": {
        "id": "bpp2d9XN_Zdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y6Tbdgk__GDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-DGRBbGxI6G"
      },
      "source": [
        "### 작은 모델 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrfoVQheYSO5"
      },
      "source": [
        "앞서 만든 기준 모델과 비교하기 위해 적은 수의 은닉 유닛을 가진 모델을 만들어 보죠:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jksi-XtaxDAh"
      },
      "outputs": [],
      "source": [
        "class Smaller_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fcn = nn.Sequential(\n",
        "            nn.Linear(1000, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.fcn(x)\n",
        "        y = torch.sigmoid(f)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbngCZliYdma"
      },
      "source": [
        "같은 데이터를 사용해 이 모델을 훈련합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofn1AwDhx-Fe"
      },
      "outputs": [],
      "source": [
        "model = Smaller_model().to(device)\n",
        "smaller_train_losses, smaller_test_losses = train_and_test(model, torch.optim.Adam(model.parameters(), lr=1e-3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIPuf23FFaVn"
      },
      "source": [
        "### 큰 모델 만들기\n",
        "\n",
        "아주 큰 모델을 만들어 얼마나 빠르게 과대적합이 시작되는지 알아 볼 수 있습니다. 이 문제에 필요한 것보다 훨씬 더 큰 용량을 가진 네트워크를 추가해서 비교해 보죠:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Bigger_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fcn = nn.Sequential(\n",
        "            nn.Linear(1000, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.fcn(x)\n",
        "        y = torch.sigmoid(f)\n",
        "        return y"
      ],
      "metadata": {
        "id": "HAfL-dZCNw6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-d-i5DaYmr7"
      },
      "source": [
        "역시 같은 데이터를 사용해 모델을 훈련합니다:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Bigger_model().to(device)\n",
        "bigger_train_losses, bigger_test_losses = train_and_test(model, torch.optim.Adam(model.parameters(), lr=1e-3))"
      ],
      "metadata": {
        "id": "offJp_-ON6hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy3CMUZpzH3d"
      },
      "source": [
        "### 훈련 손실과 검증 손실 그래프 그리기\n",
        "\n",
        "<!--TODO(markdaoust): This should be a one-liner with tensorboard -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSlo1F4xHuuM"
      },
      "source": [
        "실선은 훈련 손실이고 점선은 검증 손실입니다(낮은 검증 손실이 더 좋은 모델입니다). 여기서는 작은 네트워크가 기준 모델보다 더 늦게 과대적합이 시작되었습니다(즉 에포크 4가 아니라 6에서 시작됩니다). 또한 과대적합이 시작되고 훨씬 천천히 성능이 감소합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XmKDtOWzOpk"
      },
      "outputs": [],
      "source": [
        "def plot_history(histories, key='binary_crossentropy'):\n",
        "    plt.figure(figsize=(16,10))\n",
        "\n",
        "    for name, train_losses, test_losses in histories:\n",
        "        val = plt.plot(list(range(1,len(test_losses)+1)), test_losses, '--', label=name.title()+' Val')\n",
        "        plt.plot(list(range(1,len(train_losses)+1)), train_losses, color=val[0].get_color(), label=name.title()+' Train')\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(key.replace('_',' ').title())\n",
        "    plt.legend()\n",
        "\n",
        "    plt.xlim([0,max(list(range(1,len(train_losses)+1)))])\n",
        "    plt.show()\n",
        "\n",
        "plot_history([('baseline', baseline_train_losses, baseline_test_losses),\n",
        "              ('smaller', smaller_train_losses, smaller_test_losses),\n",
        "              ('bigger', bigger_train_losses, bigger_test_losses)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi6hBhdnSfjA"
      },
      "source": [
        "큰 네트워크는 거의 바로 첫 번째 에포크 이후에 과대적합이 시작되고 훨씬 더 심각하게 과대적합됩니다. 네트워크의 용량이 많을수록 훈련 세트를 더 빠르게 모델링할 수 있습니다(훈련 손실이 낮아집니다). 하지만 더 쉽게 과대적합됩니다(훈련 손실과 검증 손실 사이에 큰 차이가 발생합니다)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASdv7nsgEFhx"
      },
      "source": [
        "## 과대적합을 방지하기 위한 전략"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rHoVWcswFLa"
      },
      "source": [
        "### 가중치를 규제하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRxWepNawbBK"
      },
      "source": [
        "오캄의 면도날(Occam’s Razor) 원칙에 따르면, 어떤 현상을 설명하는 방법이 여러 가지라면 가장 단순한 설명이 더 정확할 가능성이 높다고 합니다. 신경망 모델도 마찬가지로, 단순한 모델일수록 과대적합 가능성이 낮아집니다.\n",
        "\n",
        "가중치 규제(Weight Regularization)란?\n",
        "신경망의 복잡도를 줄이기 위해 가중치가 너무 커지지 않도록 제약을 가하는 방법입니다.\n",
        "모델이 특정한 패턴을 지나치게 암기하는 것을 방지하고, 가중치의 분포를 균일하게 만들어 일반화를 돕습니다.\n",
        "\n",
        "가중치 규제 방법\n",
        "손실 함수에 **큰 가중치에 대한 페널티(추가 비용)**를 포함하는 방식이며, 두 가지 방법이 있습니다.\n",
        "\n",
        "* [L1 규제](https://developers.google.com/machine-learning/glossary/#L1_regularization)는 가중치의 절댓값에 비례하는 비용이 추가됩니다(즉, 가중치의 \"L1 노름(norm)\"을 추가합니다). 일부 가중치를 0으로 만들어 희소한 모델을 생성합니다.\n",
        "\n",
        "* [L2 규제](https://developers.google.com/machine-learning/glossary/#L2_regularization)는 가중치의 제곱에 비례하는 비용이 추가됩니다(즉, 가중치의 \"L2 노름\"의 제곱을 추가합니다). 가중치를 0에 가깝게 하지만, 완전히 0으로 만들지는 않습니다. L2 규제는 \"가중치 감쇠(weight decay)\"라고도 불리며, 두 개념은 같은 의미입니다.\n",
        "\n",
        "일반적으로 L2 규제가 더 자주 사용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFGmcwduwVyQ"
      },
      "outputs": [],
      "source": [
        "model = Bigger_model().to(device)\n",
        "l2_train_losses, l2_test_losses = train_and_test(model, torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.005))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUUHoXb7w-_C"
      },
      "source": [
        "```l2(0.001)```는 네트워크의 전체 손실에 층에 있는 가중치 행렬의 모든 값이 ```0.001 * weight_coefficient_value**2```만큼 더해진다는 의미입니다. 이런 페널티(penalty)는 훈련할 때만 추가됩니다. 따라서 테스트 단계보다 훈련 단계에서 네트워크 손실이 훨씬 더 클 것입니다.\n",
        "\n",
        "L2 규제의 효과를 확인해 보죠:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wkfLyxBZdh_"
      },
      "outputs": [],
      "source": [
        "plot_history([('biggermodel', bigger_train_losses, bigger_test_losses),\n",
        "              ('l2', l2_train_losses, l2_test_losses)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx1YHMsVxWjP"
      },
      "source": [
        "결과에서 보듯이 모델 파라미터의 개수는 같지만 L2 규제를 적용한 모델이 기본 모델보다 과대적합에 훨씬 잘 견디고 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmnBNOOVxiG8"
      },
      "source": [
        "### 드롭아웃 추가하기\n",
        "\n",
        "드롭아웃(dropout)은 신경망에서 가장 효과적이고 널리 사용하는 규제 기법 중 하나입니다. 토론토(Toronto) 대학의 힌튼(Hinton)과 그의 제자들이 개발했습니다. 드롭아웃을 층에 적용하면 훈련하는 동안 층의 출력 특성을 랜덤하게 끕니다(즉, 0으로 만듭니다). 훈련하는 동안 어떤 입력 샘플에 대해 [0.2, 0.5, 1.3, 0.8, 1.1] 벡터를 출력하는 층이 있다고 가정해 보죠. 드롭아웃을 적용하면 이 벡터에서 몇 개의 원소가 랜덤하게 0이 됩니다. 예를 들면, [0, 0.5, 1.3, 0, 1.1]가 됩니다. \"드롭아웃 비율\"은 0이 되는 특성의 비율입니다. 보통 0.2에서 0.5 사이를 사용합니다. 테스트 단계에서는 어떤 유닛도 드롭아웃하지 않습니다. 훈련 단계보다 더 많은 유닛이 활성화되기 때문에 균형을 맞추기 위해 층의 출력 값을 드롭아웃 비율만큼 줄입니다.\n",
        "\n",
        "네트워크에 두 개의 `Dropout` 층을 추가하여 과대적합이 얼마나 감소하는지 알아 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFEYvtrHxSWS"
      },
      "outputs": [],
      "source": [
        "class Dropout_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fcn = nn.Sequential(\n",
        "            nn.Linear(1000, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.75),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.75),\n",
        "            nn.Linear(512, 1),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.fcn(x)\n",
        "        y = torch.sigmoid(f)\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Dropout_model().to(device)\n",
        "dpt_train_losses, dpt_test_losses = train_and_test(model, torch.optim.Adam(model.parameters(), lr=1e-3))"
      ],
      "metadata": {
        "id": "R2k37t10TwPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPZqwVchx5xp"
      },
      "outputs": [],
      "source": [
        "plot_history([('biggermodel', bigger_train_losses, bigger_test_losses),\n",
        "              ('dropout', dpt_train_losses, dpt_test_losses)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjfnkEeQyAFG"
      },
      "source": [
        "드롭아웃을 추가하니 기준 모델보다 확실히 향상되었습니다.\n",
        "\n",
        "정리하면 신경망에서 과대적합을 방지하기 위해 가장 널리 사용하는 방법은 다음과 같습니다:\n",
        "\n",
        "* 더 많은 훈련 데이터를 모읍니다.\n",
        "* 네트워크의 용량을 줄입니다.\n",
        "* 가중치 규제를 추가합니다.\n",
        "* 드롭아웃을 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O94lsQ_jrVkm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}